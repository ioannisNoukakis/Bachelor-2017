STEP one build VGG16 FT and train it
-> very bad results [[14.544812317192108, 0.097609762795711347], [14.532413921921263, 0.098378984400218528], [14.544812317192108, 0.097609762795711347], [14.532413921921263, 0.098378984400218528], [14.544812317192108, 0.097609762795711347], [14.532413921921263, 0.098378984400218528], [14.544812317192108, 0.097609762795711347], [14.532413921921263, 0.098378984400218528], [14.544812317192108, 0.097609762795711347], [14.532413921921263, 0.098378984400218528], [14.544812317192108, 0.097609762795711347], [14.532413921921263, 0.098378984400218528], [14.544812317192108, 0.097609762795711347], [14.532413921921263, 0.098378984400218528], [14.544812317192108, 0.097609762795711347], [14.532413921921263, 0.098378984400218528], [14.544812317192108, 0.097609762795711347], [14.532413921921263, 0.098378984400218528], [14.544812317192108, 0.097609762795711347], [14.532413921921263, 0.098378984400218528], [14.544812317192108, 0.097609762795711347], [14.532413921921263, 0.098378984400218528], [14.544812317192108, 0.097609762795711347], [14.532413921921263, 0.098378984400218528], [14.544812317192108, 0.097609762795711347], [14.532413921921263, 0.098378984400218528], [14.544812317192108, 0.097609762795711347], [14.532413921921263, 0.098378984400218528], [14.544812317192108, 0.097609762795711347], [14.532413921921263, 0.098378984400218528]]

STEP Two: train VGG16 with classic architecture and then load, remove fully connected
add GAP and do more training then draw CAMs.

But the fully connected does even worse! [VGG16_FT] [[15.806985176781533, 0.019301930480640474], [15.789246738789933, 0.020402459799410445], [15.826328813606459, 0.018101810450756092], [15.823104876520061, 0.018301830455736825], [15.790865470366617, 0.020302030505544126], [15.868240028157784, 0.015501550386006599], [15.853864450923732, 0.016393442867232152], [15.806985176781533, 0.019301930480640474], [15.789246738789933, 0.020402459799410445], [15.826328813606459, 0.018101810450756092], [15.823104876520061, 0.018301830455736825], [15.790865470366617, 0.020302030505544126], [15.868240028157784, 0.015501550386006599], [15.853864450923732, 0.016393442867232152], [15.806985176781533, 0.019301930480640474], [15.789246738789933, 0.020402459799410445], [15.826328813606459, 0.018101810450756092], [15.823104876520061, 0.018301830455736825], [15.790865470366617, 0.020302030505544126], [15.868240028157784, 0.015501550386006599], [15.853864450923732, 0.016393442867232152], [15.806985176781533, 0.019301930480640474], [15.789246738789933, 0.020402459799410445], [15.826328813606459, 0.018101810450756092], [15.823104876520061, 0.018301830455736825], [15.790865470366617, 0.020302030505544126], [15.868240028157784, 0.015501550386006599], [15.853864450923732, 0.016393442867232152], [15.806985176781533, 0.019301930480640474], [15.789246738789933, 0.020402459799410445], [15.826328813606459, 0.018101810450756092], [15.823104876520061, 0.018301830455736825], [15.790865470366617, 0.020302030505544126], [15.868240028157784, 0.015501550386006599], [15.853864450923732, 0.016393442867232152], [15.806985176781533, 0.019301930480640474], [15.789246738789933, 0.020402459799410445], [15.826328813606459, 0.018101810450756092], [15.823104876520061, 0.018301830455736825], [15.790865470366617, 0.020302030505544126], [15.868240028157784, 0.015501550386006599], [15.853864450923732, 0.016393442867232152], [15.806985176781533, 0.019301930480640474], [15.789246738789933, 0.020402459799410445], [15.826328813606459, 0.018101810450756092], [15.823104876520061, 0.018301830455736825], [15.790865470366617, 0.020302030505544126], [15.868240028157784, 0.015501550386006599], [15.853864450923732, 0.016393442867232152], [15.806985176781533, 0.019301930480640474], [15.789246738789933, 0.020402459799410445]]

HEre are the finals results over 5 epochs
[[0.11920067143452988, 0.96319631371859105], [0.10552861226166615, 0.96534376187055315], [0.11028753402932762, 0.96679667437454497], [0.10554082990884833, 0.96646170578834534], [0.047632323303149232, 0.98609860696391061], [0.04933760498798271, 0.98462828079237497], [0.043999131675324411, 0.98509850683468847], [0.037145160485338787, 0.9863051955029577], [0.037988017091219108, 0.98849884750056427], [0.040775122291613924, 0.98742313858781727]]

ANd here we saved the model rather than its weights.
[VGG16_FT] [[0.15311012780484412, 0.95029502307692704], [0.1633368680285055, 0.94885410357614419], [0.094951108095151435, 0.96799679467267707], [0.11326946338427893, 0.96003353375839484], [0.065263293097610295, 0.97899789639217449], [0.07676402714317046, 0.9768026793649438], [0.054419325210887617, 0.9844984468644542], [0.064491330001441152, 0.98211290880979407], [0.027759057838650559, 0.99119911811175088], [0.030645811768909466, 0.99049748236270774]]

So let's try again with basic GAP architecture.

STEP tree with all 4 background and their combinaisons

STEP four bias metrics with mohanty

STEP 5 analyse results

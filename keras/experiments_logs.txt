STEP one build VGG16 FT and train it
-> very bad results [[14.544812317192108, 0.097609762795711347], [14.532413921921263, 0.098378984400218528], [14.544812317192108, 0.097609762795711347], [14.532413921921263, 0.098378984400218528], [14.544812317192108, 0.097609762795711347], [14.532413921921263, 0.098378984400218528], [14.544812317192108, 0.097609762795711347], [14.532413921921263, 0.098378984400218528], [14.544812317192108, 0.097609762795711347], [14.532413921921263, 0.098378984400218528], [14.544812317192108, 0.097609762795711347], [14.532413921921263, 0.098378984400218528], [14.544812317192108, 0.097609762795711347], [14.532413921921263, 0.098378984400218528], [14.544812317192108, 0.097609762795711347], [14.532413921921263, 0.098378984400218528], [14.544812317192108, 0.097609762795711347], [14.532413921921263, 0.098378984400218528], [14.544812317192108, 0.097609762795711347], [14.532413921921263, 0.098378984400218528], [14.544812317192108, 0.097609762795711347], [14.532413921921263, 0.098378984400218528], [14.544812317192108, 0.097609762795711347], [14.532413921921263, 0.098378984400218528], [14.544812317192108, 0.097609762795711347], [14.532413921921263, 0.098378984400218528], [14.544812317192108, 0.097609762795711347], [14.532413921921263, 0.098378984400218528], [14.544812317192108, 0.097609762795711347], [14.532413921921263, 0.098378984400218528]]

STEP Two: train VGG16 with classic architecture and then load, remove fully connected
add GAP and do more training then draw CAMs.

But the fully connected does even worse! [VGG16_FT] [[15.806985176781533, 0.019301930480640474], [15.789246738789933, 0.020402459799410445], [15.826328813606459, 0.018101810450756092], [15.823104876520061, 0.018301830455736825], [15.790865470366617, 0.020302030505544126], [15.868240028157784, 0.015501550386006599], [15.853864450923732, 0.016393442867232152], [15.806985176781533, 0.019301930480640474], [15.789246738789933, 0.020402459799410445], [15.826328813606459, 0.018101810450756092], [15.823104876520061, 0.018301830455736825], [15.790865470366617, 0.020302030505544126], [15.868240028157784, 0.015501550386006599], [15.853864450923732, 0.016393442867232152], [15.806985176781533, 0.019301930480640474], [15.789246738789933, 0.020402459799410445], [15.826328813606459, 0.018101810450756092], [15.823104876520061, 0.018301830455736825], [15.790865470366617, 0.020302030505544126], [15.868240028157784, 0.015501550386006599], [15.853864450923732, 0.016393442867232152], [15.806985176781533, 0.019301930480640474], [15.789246738789933, 0.020402459799410445], [15.826328813606459, 0.018101810450756092], [15.823104876520061, 0.018301830455736825], [15.790865470366617, 0.020302030505544126], [15.868240028157784, 0.015501550386006599], [15.853864450923732, 0.016393442867232152], [15.806985176781533, 0.019301930480640474], [15.789246738789933, 0.020402459799410445], [15.826328813606459, 0.018101810450756092], [15.823104876520061, 0.018301830455736825], [15.790865470366617, 0.020302030505544126], [15.868240028157784, 0.015501550386006599], [15.853864450923732, 0.016393442867232152], [15.806985176781533, 0.019301930480640474], [15.789246738789933, 0.020402459799410445], [15.826328813606459, 0.018101810450756092], [15.823104876520061, 0.018301830455736825], [15.790865470366617, 0.020302030505544126], [15.868240028157784, 0.015501550386006599], [15.853864450923732, 0.016393442867232152], [15.806985176781533, 0.019301930480640474], [15.789246738789933, 0.020402459799410445], [15.826328813606459, 0.018101810450756092], [15.823104876520061, 0.018301830455736825], [15.790865470366617, 0.020302030505544126], [15.868240028157784, 0.015501550386006599], [15.853864450923732, 0.016393442867232152], [15.806985176781533, 0.019301930480640474], [15.789246738789933, 0.020402459799410445]]

HEre are the finals results over 5 epochs
[[0.11920067143452988, 0.96319631371859105], [0.10552861226166615, 0.96534376187055315], [0.11028753402932762, 0.96679667437454497], [0.10554082990884833, 0.96646170578834534], [0.047632323303149232, 0.98609860696391061], [0.04933760498798271, 0.98462828079237497], [0.043999131675324411, 0.98509850683468847], [0.037145160485338787, 0.9863051955029577], [0.037988017091219108, 0.98849884750056427], [0.040775122291613924, 0.98742313858781727]]

ANd here we saved the model rather than its weights.
[VGG16_FT] [[0.15059231647470822, 0.955295523126932], [0.17338450570383082, 0.94214644480044785], [0.10067342008284623, 0.96909690500557544], [0.11132651846616431, 0.96310787620059335], [0.04224727773656075, 0.98509850689429901], [0.044259932781494989, 0.9851872524847326], [0.054093553566255996, 0.98279827662093222], [0.065893511065185248, 0.97708216431155559], [0.11487178353692709, 0.9666966642304794], [0.13166247157077818, 0.96366684765972987]]
[VGG16_FT]

AND HERE IS THE RANDOM TRAINING
[VGG16_FT] [[0.1690408252347452, 0.94979497301231586], [0.17389165184427002, 0.9499720459946579], [0.072490831835004585, 0.97539753484206149], [0.080022227208136662, 0.97484627834174009], [0.067296598299213195, 0.97879787574637978], [0.066633740675966446, 0.97820010759631904], [0.065196735043652143, 0.97859785587552284], [0.051657606430470356, 0.98351033737434235], [0.03626556103762027, 0.98999899769439759], [0.023244214046559467, 0.99301285467846157]]
[VGG16_FT]

So let's try again with basic GAP architecture.

STEP tree with all 4 background and their combinaisons

STEP four bias metrics with mohanty

STEP 5 analyse results
